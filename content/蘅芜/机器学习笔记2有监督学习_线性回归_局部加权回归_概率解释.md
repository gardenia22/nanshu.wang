---
date: 2015-02-11
description: ""
tags: ["机器学习","有监督学习","线性回归","局部加权回归","概率解释"]
title: "机器学习笔记2 有监督学习 线性回归 局部加权回归 概率解释"
topics: []
draft: false
url: /post/2015-02-11
---
Andrew Ng cs229 Machine Learning 笔记

# 有监督学习

## 局部加权线性回归(Locally weighted linear regression)

参数学习算法(parametric learning algorithm)：参数个数固定

非参数学习算法(non-parametric learning algorithm)：参数个数随样本增加

特征选择对参数学习算法非常重要，否则会出现下面的问题：

* 欠拟合(underfitting)：特征过少，模型过于简单，高偏差(high bias)，不能很好拟合训练集
* 过拟合(overfitting)：特征过多，模型过于复杂，高方差(high variance)，过于拟合训练集，不能很好预测新样本

对于非参数学习算法来说，并不需要进行精心的特征选择，局部加权线性回归就是这样。

局部加权回归又叫做Loess，其成本函数为：
<!--more-->

<div>
$$\sum_iw^{(i)}(y^{(i)}-\theta^Tx^{(i)})^2$$
</div>

$w^{(i)}$是非负权重，通常定义$w^{(i)}$如下：

<div>
$$w^{(i)}=exp(- \frac{(x^{(i)}-x)^2}{2\tau^2})$$
</div>

权值取决于用于预测的输入变量$x$的值：离$x$越近的样本，权值越接近1；越远的样本，越接近0。$\tau$被称为带宽(bandwidth)参数，决定了以$x$为中心，样本权重递减的速度。$\tau$越大，递减速度越慢；$\tau$越小，递减速度越快。

参数学习算法的参数是固定的，一经学习得到不再改变。而非参数学习算法的参数并不固定，每次预测都要重新学习一组新的参数，并且要一直保留完整的训练样本集。当样本集很大时，局部加权回归的计算开销会很大，Andrew Moore提出的KD-tree方法可以在大数据集上的计算更高效。

## 回归模型的概率解释

遇到一个回归问题，为什么采用线性回归，又为什么采用最小二乘作为成本函数？其实最小二乘回归中蕴含了非常自然的概率假设。假设目标值和输入值之间满足以下关系：

<div>
$$y^{(i)}=\theta^Tx^{(i)}+\epsilon^{(i)}$$
</div>

其中，$\epsilon^{(i)}$是误差项，包含了模型未考虑到的影响目标值的因素和随机噪声。假设误差项独立同分布，服从均值为0，方差为$\sigma^2$的高斯分布（正态分布），即$\epsilon^{(i)}\sim N(0,\sigma^2)$。$\epsilon^{(i)}$的密度函数为：

<div>
$$p(\epsilon^{(i)})=\frac1{\sqrt{2\pi}\sigma}exp(-\frac{(\epsilon^{(i)})^2}{2\sigma^2})$$
</div>

也就是说：

<div>
$$p(y^{(i)}|x^{(i)};\theta)=\frac1{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})$$
</div>

或者也可以这样写：$(y^{(i)}|x^{(i)};\theta) \sim N(\theta^Tx^{(i)},\sigma^2)$。

为什么采用正态分布？一种原因是数学上处理起来比较便利，二是因为根据中心极限定理，独立的随机变量的和，即多种随机误差的累积，其总的影响是接近正态分布的。

$p(y^{(i)}|x^{(i)};\theta)$的含义是给定条件$x^{(i)}$，参数设定为$\theta$时，$y^{(i)}$的分布值。不能将$\theta$看作是概率条件，因为$\theta$不是随机变量，而是实际存在的真实值，虽然我们不知道真实值到底是多少。因此在以上的式子中，用了分号而不是逗号来区分$x^{(i)}$和$\theta$。

定义$\theta$的似然(likelihood)函数：

<div>
$$\begin{equation}
\begin{split}
L(\theta)=& p(y|X;\theta)\\
=& \Pi_{i=1}^m p(y^{(i)}|x^{(i)};\theta)\\
=& \Pi_{i=1}^m \frac1{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})
\end{split}
\end{equation}$$
</div>

似然(likelihood)和概率(probability)实际上是一个东西，但是似然函数是对参数$\theta$定义的，为了加以区分，使用了似然这一术语。我们可以说参数的似然，数据的概率，但不能说数据的似然，参数的概率。

极大似然估计的含义是选择参数$\theta$，使参数的似然函数最大化，也就是说选择参数使得已有样本数据出现的概率最大。

为方便求解，再定义函数$l(\theta)$：

<div>
$$\begin{equation}
\begin{split}
l(\theta) =& \log L(\theta) \\
=& \sum_{i=1}^m \log \frac1{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})\\
=& m\log (\frac1{\sqrt{2\pi}\sigma}-\frac1{\sigma^2}\frac12\sum_{i=1}^m(y^{(i)}-\theta^Tx^{(i)})^2)
\end{split}
\end{equation}$$
</div>

可得，要最大化似然函数$L(\theta)$，也就是最大化$l(\theta)$，也就是最小化：

<div>
$$\frac12\sum_{i=1}^m(y^{(i)}-\theta^Tx^{(i)})^2$$
</div>

这个式子刚好是最小二乘法中定义的成本函数$J(\theta)$。总结一下，最小二乘回归模型刚好就是在假设了误差独立同服从正态分布后，得到的最大似然估计。注意到，正态分布中的方差$\sigma^2$的取值对模型并没有影响。

